### **FlowMind AI \- Complete Technical Proposal**

**(End-to-End Implementation Guide)**

---

### **1\. Core Architecture Overview**

**FlowMind AI** is designed as a proactive personal productivity assistant, built upon Bolt.new's robust platform. Its core architecture emphasizes a **multi-agent AI system**, where intelligence and automation are delivered with minimal custom code. This approach maximizes efficiency and scalability by leveraging Bolt.new's native capabilities and advanced **prompt engineering**.

Fragmento de cÃ³digo

graph LR  
    A\[User Interface\<br\>(Bolt.new Frontend)\] \--\> B\[Bolt.new Engine\<br\>(Core Logic & APIs)\]  
    B \--\> C\[Agent Orchestrator\<br\>(MindFlow Agent)\]  
    C \--\> D\[TaskFlow Agent\<br\>(Prompt-Driven)\]  
    C \--\> E\[CalendarFlow Agent\<br\>(Prompt-Driven)\]  
    C \--\> F\[InfoFlow Agent\<br\>(Prompt-Driven)\]  
    B \--\> G\[PostgreSQL DB\<br\>(Managed by Prisma)\]  
    B \--\> H\[Google APIs\<br\>(Calendar/Tasks)\]  
    B \--\> I\[LLM Gateway\<br\>(Dynamic Model Switching)\]  
    I \--\> J\[OpenAI/Gemini/Claude\]  
    B \--\> K\[ElevenLabs API\<br\>(Voice AI Integration)\]  
    B \--\> L\[Tavus API\<br\>(Video AI Integration)\]  
    B \--\> M\[Supabase\<br\>(External Auth & Scaling)\]  
    B \--\> N\[Netlify\<br\>(Automated Deployment)\]

---

### **2\. Detailed File Structure**

This file structure highlights our **minimal-code strategy**, with Bolt.new handling much of the heavy lifting. Core logic is primarily driven by declarative prompt files, complemented by concise JavaScript integrations only where absolutely necessary for external API communication.

Bash

flowmind-ai/  
â”œâ”€â”€ bolt.new/                  \# Bolt.new generated core application files  
â”‚   â”œâ”€â”€ ui/                    \# Frontend UI components generated by Bolt.new  
â”‚   â”‚   â”œâ”€â”€ dashboard/         \# Main user dashboard interface  
â”‚   â”‚   â”‚   â”œâ”€â”€ index.html     \# Core dashboard layout, unifying information  
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.html      \# Central conversational chat interface  
â”‚   â”‚   â”‚   â”œâ”€â”€ tasks.html     \# Dedicated section for task management display  
â”‚   â”‚   â”‚   â””â”€â”€ calendar.html  \# Integrated mini-calendar view  
â”‚   â”‚   â”‚  
â”‚   â”‚   â””â”€â”€ auth/  
â”‚   â”‚       â””â”€â”€ google.html    \# Google OAuth login interface (Bolt.new's integrated component)  
â”‚   â”‚  
â”‚   â”œâ”€â”€ server/                \# Backend logic and API integrations  
â”‚   â”‚   â”œâ”€â”€ models/            \# Prisma schema definitions for the PostgreSQL database  
â”‚   â”‚   â”‚   â”œâ”€â”€ Task.prisma    \# Defines the Task data model  
â”‚   â”‚   â”‚   â”œâ”€â”€ Event.prisma   \# Defines the Calendar Event data model  
â”‚   â”‚   â”‚   â”œâ”€â”€ Note.prisma    \# Defines the user Note data model  
â”‚   â”‚   â”‚   â””â”€â”€ LLMConfig.prisma \# Stores configuration for dynamic LLM endpoints  
â”‚   â”‚   â”‚  
â”‚   â”‚   â”œâ”€â”€ agents/            \# Core AI agent logic defined through natural language prompts  
â”‚   â”‚   â”‚   â”œâ”€â”€ TaskFlow.prompt    \# Agent responsible for task management operations  
â”‚   â”‚   â”‚   â”œâ”€â”€ CalendarFlow.prompt\# Agent for managing calendar events  
â”‚   â”‚   â”‚   â”œâ”€â”€ InfoFlow.prompt    \# Agent for information retrieval and summarization  
â”‚   â”‚   â”‚   â””â”€â”€ MindFlow.prompt    \# The proactive orchestrator agent, central to FlowMind AI  
â”‚   â”‚   â”‚  
â”‚   â”‚   â”œâ”€â”€ integrations/      \# Minimal custom JavaScript for essential external API calls  
â”‚   â”‚   â”‚   â”œâ”€â”€ google.js      \# Handles communication with Google Calendar/Tasks APIs  
â”‚   â”‚   â”‚   â”œâ”€â”€ elevenlabs.js  \# Dedicated logic for ElevenLabs Voice AI integration  
â”‚   â”‚   â”‚   â””â”€â”€ tavus.js       \# Handles interactions with Tavus Video AI API  
â”‚   â”‚   â”‚  
â”‚   â”‚   â””â”€â”€ utils/             \# Utility functions  
â”‚   â”‚       â”œâ”€â”€ llmRouter.js   \# Manages dynamic switching between different LLM providers  
â”‚   â”‚       â””â”€â”€ scheduler.js   \# Handles cron-like jobs for proactive agent triggers  
â”‚   â”‚  
â”‚   â””â”€â”€ hooks/                 \# Bolt.new lifecycle hooks for specific application events  
â”‚       â”œâ”€â”€ postLogin.js       \# Executes logic immediately after user Google OAuth login (e.g., setting up API access)  
â”‚       â””â”€â”€ cron.js            \# Defines and schedules background tasks for agents  
â”‚  
â”œâ”€â”€ config/  
â”‚   â”œâ”€â”€ netlify.toml           \# Configuration file for Netlify deployment  
â”‚   â””â”€â”€ supabase.json          \# Configuration for connecting to Supabase as an external database  
â”‚  
â”œâ”€â”€ tests/                     \# Automated test suite  
â”‚   â”œâ”€â”€ agents.test.js         \# Unit/integration tests for prompt-driven agent logic  
â”‚   â””â”€â”€ integrations.test.js   \# Tests for external API integrations (Google, ElevenLabs, Tavus)  
â”‚  
â””â”€â”€ .env                       \# Environment variables for API keys and other sensitive configurations

---

### **3\. Technology Stack Implementation**

FlowMind AI leverages Bolt.new's powerful full-stack capabilities, strategically integrating external services to meet all challenge requirements with **minimal custom code**.

| Component | Technology | Implementation Details | Low-Code Approach & Challenge Alignment |
| :---- | :---- | :---- | :---- |
| **Core Platform** | **Bolt.new** | Rapid web app generation from natural language prompts, full-stack orchestration. | The entire application scaffold, most UI, and backend boilerplate are generated via **simple prompts**. Core logic is embedded within prompts. |
| **UI Framework** | **React (Bolt-compiled)** | Automatic component generation, dynamic routing. | Bolt.new translates high-level UI prompts directly into functional React components, eliminating manual component creation. |
| **State Management** | **Bolt Context API** | Built-in, efficient state handling for seamless data flow across components. | Leverages Bolt.new's native state management, abstracting complex state logic. |
| **Database** | **PostgreSQL \+ Prisma** | Robust relational database with powerful ORM for data persistence. | **Schema defined by natural language prompts**; Bolt.new handles Prisma setup, migrations, and database interactions automatically. |
| **Authentication** | **Google OAuth 2.0** | Secure user login via Gmail. Simplifies user onboarding. | **1-click integration** directly supported by Bolt.new, handling tokens and user sessions without custom authentication code. |
| **AI Orchestration** | **Multi-LLM Router** | Dynamic switching between LLM providers (OpenAI, Google Gemini, Anthropic Claude). | **Minimal llmRouter.js** uses a simple database entry (LLMConfig.prisma) to determine the active LLM, routing requests without code changes. |
| **Voice Interface** | **ElevenLabs API** | Speech-to-Text (STT) for input and Text-to-Speech (TTS) for output. | **Voice AI Challenge**: \*\*\<50 LoC in elevenlabs.js\` \*\* handles API calls. UI buttons trigger audio capture/playback, transcibing inputs and synthesizing responses. |
| **Monetization** | **RevenueCat (Web Strategy)** | Subscription management for premium features. | **Make More Money Challenge**: While primarily mobile, we'll implement a PWA-friendly "premium plan" if Bolt.new supports a light web SDK for RevenueCat, or by handling web hooks from RevenueCat directly. |

---

### **4\. Core Agent Logic Implementation**

Our agents are the heart of FlowMind AI's intelligence, implemented almost entirely through **declarative prompts** within Bolt.new, significantly reducing code.

#### **4.1 TaskFlow Agent (server/agents/TaskFlow.prompt)**

This agent manages the lifecycle of tasks based on natural language commands.

Fragmento de cÃ³digo

WHEN user\_input MATCHES:  
  "add task {task\_name} for {date}" â†’  
    CREATE Task(title: "{task\_name}", due\_date: "{date}", status: "pending", user\_id: current\_user.id)  
    RESPONSE "âœ… Task '{task\_name}' added for {date}."

  "complete {task\_name}" â†’  
    UPDATE Task SET status \= "done" WHERE title \= "{task\_name}" AND user\_id \= current\_user.id  
    RESPONSE "ðŸŽ‰ Great job\! '{task\_name}' marked as complete."

  "delete {task\_name}" â†’  
    DELETE Task WHERE title \= "{task\_name}" AND user\_id \= current\_user.id  
    RESPONSE "ðŸ—‘ï¸ Task '{task\_name}' has been deleted."

  "show my tasks" â†’  
    QUERY tasks \= SELECT \* FROM Task WHERE user\_id \= current\_user.id AND status \!= "done" ORDER BY due\_date ASC  
    IF tasks IS EMPTY THEN RESPONSE "You have no pending tasks."  
    ELSE RESPONSE "Here are your tasks: {tasks.map(t \=\> \`${t.title} (due ${t.due\_date})\`).join(', ')}"

#### **4.2 MindFlow Agent (server/agents/MindFlow.prompt)**

This is the **proactive orchestrator**, analyzing user data to provide intelligent suggestions and automate workflows. Its logic runs on a scheduled basis.

Fragmento de cÃ³digo

EVERY 30 MINUTES FOR EACH USER:  
  // 1\. Identify urgent tasks  
  QUERY urgent\_tasks \= SELECT \* FROM Task  
    WHERE user\_id \= current\_user.id  
    AND due\_date \< NOW() \+ 24h  
    AND priority IN ('high','medium')  
    AND status \= 'pending'

  // 2\. Find available time slots in Google Calendar  
  QUERY free\_slots \= CALL google\_api.get\_free\_calendar\_slots(  
    user\_id: current\_user.id,  
    start\_time: NOW(),  
    end\_time: NOW() \+ 24h,  
    min\_duration: '1h'  
  ) // This would map to the google.js integration

  // 3\. Propose scheduling urgent tasks into free slots  
  FOR EACH task IN urgent\_tasks:  
    FOR EACH slot IN free\_slots:  
      IF slot.duration \>= task.estimated\_time THEN  
        SUGGEST\_UI "ðŸ’¡ How about working on '{task.title}' at {slot.start} (for {task.estimated\_time})?"  
        WITH ACTIONS \["Confirm", "Later", "Dismiss"\]  
          ON "Confirm":  
            CREATE Event(  
              title: "Work on {task.title}",  
              start\_time: slot.start,  
              end\_time: slot.start \+ task.estimated\_time,  
              user\_id: current\_user.id  
            ) // Maps to google.js createCalendarEvent  
            UPDATE Task SET status \= 'scheduled' WHERE id \= task.id  
            RESPONSE "Great\! '{task.title}' has been scheduled for you."  
          ON "Later":  
            RESPONSE "Okay, I'll remind you about '{task.title}' again later."  
          ON "Dismiss":  
            RESPONSE "Understood. I won't suggest that again for this task today."

---

### **5\. Critical Integration Code**

These small, focused JavaScript files handle the communication layer with external APIs, orchestrated by the Bolt.new engine and our prompt-driven agents.

#### **5.1 Dynamic LLM Switching (server/utils/llmRouter.js)**

This compact function allows FlowMind AI to switch between different LLM providers based on a database configuration, offering flexibility and potential cost optimization.

JavaScript

// This file runs within Bolt.new's server environment.  
const prisma \= require('../models'); // Bolt.new provides Prisma ORM access

const routeLLM \= async (promptMessages) \=\> {  
  // Fetch the currently active LLM configuration from the database  
  const config \= await prisma.lLMConfig.findFirst({  
    where: { is\_active: true }  
  });

  if (\!config) {  
    throw new Error("No active LLM configuration found.");  
  }

  // Prepare payload based on standard chat completion format  
  const payload \= {  
    model: config.model\_name,  
    messages: promptMessages // Expects \[{ role: "user", content: "..." }\]  
  };

  // Make the API call to the configured LLM endpoint  
  const response \= await fetch(config.endpoint, {  
    method: "POST",  
    headers: {  
      "Content-Type": "application/json",  
      "Authorization": \`Bearer ${config.api\_key}\` // Dynamically uses the stored API key  
    },  
    body: JSON.stringify(payload)  
  });

  if (\!response.ok) {  
    const errorBody \= await response.text();  
    console.error(\`LLM API error from ${config.model\_name}:\`, errorBody);  
    throw new Error(\`LLM API request failed: ${response.status} \- ${errorBody}\`);  
  }

  const data \= await response.json();  
  // Extract the LLM's response message  
  return data.choices\[0\].message.content;  
};

module.exports \= { routeLLM };

#### **5.2 Google API Integration (server/integrations/google.js)**

Leverages Bolt.new's built-in Google OAuth support to interact with user's Calendar and Tasks.

JavaScript

// This file runs within Bolt.new's server environment.  
const { google } \= require('bolt-google-auth'); // Bolt.new's utility for Google APIs

/\*\*  
 \* Creates a calendar event on the user's primary Google Calendar.  
 \* @param {string} userId \- The ID of the authenticated user.  
 \* @param {object} eventDetails \- Object containing event details (title, start, end, etc.).  
 \* @returns {Promise\<object\>} The created event object.  
 \*/  
exports.createCalendarEvent \= async (userId, eventDetails) \=\> {  
  // Obtain the Google OAuth token for the given user, managed by Bolt.new  
  const auth \= await google.getToken(userId);

  // Use the Google Calendar API client provided by Bolt.new  
  const calendar \= google.calendar('v3');  
    
  const resource \= {  
    summary: eventDetails.title,  
    start: { dateTime: eventDetails.start\_time || eventDetails.start, timeZone: 'America/Guayaquil' }, // Default to local timezone  
    end: { dateTime: eventDetails.end\_time || eventDetails.end, timeZone: 'America/Guayaquil' },  
    // Add more event details as needed (e.g., description, location)  
  };

  try {  
    const response \= await calendar.events.insert({  
      auth: auth,  
      calendarId: 'primary', // Target the user's primary calendar  
      resource: resource  
    });  
    return response.data; // Return the created event data  
  } catch (error) {  
    console.error('Error creating Google Calendar event:', error.message);  
    throw new Error('Failed to create Google Calendar event.');  
  }  
};

/\*\*  
 \* Fetches free time slots from the user's Google Calendar.  
 \* This is a simplified example; a real implementation would use free/busy queries.  
 \* @param {string} userId \- The ID of the authenticated user.  
 \* @param {Date} startTime \- Start time for the query.  
 \* @param {Date} endTime \- End time for the query.  
 \* @returns {Promise\<Array\<object\>\>} A list of dummy free slots (for MVP).  
 \*/  
exports.getFreeCalendarSlots \= async (userId, startTime, endTime, minDuration) \=\> {  
  // In a real scenario, this would use google.calendar.freebusy.query  
  // For MVP, simulate free slots or fetch and filter existing events for gaps.  
  console.log(\`Fetching free slots for user ${userId} between ${startTime} and ${endTime}\`);  
  // Dummy data for hackathon MVP to simulate available slots  
  return \[  
    { start: new Date(startTime.getTime() \+ 2 \* 60 \* 60 \* 1000).toISOString(), duration: 2 \* 60 \* 60 \* 1000 }, // 2 hours later  
    { start: new Date(startTime.getTime() \+ 5 \* 60 \* 60 \* 1000).toISOString(), duration: 1.5 \* 60 \* 60 \* 1000 }, // 5 hours later  
  \].filter(slot \=\> slot.duration \>= parseDuration(minDuration));  
};

function parseDuration(durationStr) {  
    const match \= durationStr.match(/(\\d+)(h|m)/);  
    if (match) {  
        const value \= parseInt(match\[1\]);  
        const unit \= match\[2\];  
        if (unit \=== 'h') return value \* 60 \* 60 \* 1000;  
        if (unit \=== 'm') return value \* 60 \* 1000;  
    }  
    return 0; // Default or error case  
}

#### **5.3 Voice Interface (server/integrations/elevenlabs.js)**

Enables conversational interactions by converting spoken input to text and agent responses back to speech.

JavaScript

// This file runs within Bolt.new's server environment for backend logic,  
// but some parts might be exposed via Bolt.new's generated frontend for direct client-side use.

// Assumes ELEVENLABS\_KEY is set in .env and accessible via process.env  
const ELEVENLABS\_API\_KEY \= process.env.ELEVENLABS\_KEY;  
const DEFAULT\_VOICE\_ID \= 'EXAVITQu4vr4xnSDxMaL'; // A default ElevenLabs voice ID

/\*\*  
 \* Converts audio blob to text using ElevenLabs Speech-to-Text API.  
 \* This function is likely called from the Bolt.new frontend's chat.html via a Bolt-generated endpoint.  
 \* @param {Blob} audioBlob \- The audio recording from the user's microphone.  
 \* @returns {Promise\<string\>} The transcribed text.  
 \*/  
exports.handleVoiceInput \= async (audioBlob) \=\> {  
  const formData \= new FormData();  
  formData.append('audio', audioBlob, 'audio.wav'); // Append audio blob

  try {  
    const response \= await fetch('https://api.elevenlabs.io/v1/speech-to-text', {  
      method: 'POST',  
      headers: {  
        'xi-api-key': ELEVENLABS\_API\_KEY,  
        // 'Content-Type': 'audio/wav' is often not needed with FormData as it sets automatically  
      },  
      body: formData // Send the FormData object  
    });

    if (\!response.ok) {  
      const errorBody \= await response.text();  
      console.error('ElevenLabs STT error:', errorBody);  
      throw new Error(\`Failed to transcribe audio: ${response.status} \- ${errorBody}\`);  
    }

    const data \= await response.json();  
    return data.text; // Return the transcribed text  
  } catch (error) {  
    console.error('Error in ElevenLabs STT:', error);  
    throw error;  
  }  
};

/\*\*  
 \* Generates an HTML audio tag to play text as speech using ElevenLabs Text-to-Speech API.  
 \* This is designed to be embedded directly into Bolt.new's generated UI.  
 \* @param {string} text \- The text to be spoken.  
 \* @param {string} voiceId \- Optional; the voice ID to use. Defaults to DEFAULT\_VOICE\_ID.  
 \* @returns {string} An HTML \`\<audio\>\` tag with the ElevenLabs TTS URL.  
 \*/  
exports.speakResponse \= (text, voiceId \= DEFAULT\_VOICE\_ID) \=\> {  
  const encodedText \= encodeURIComponent(text);  
  const audioUrl \= \`https://api.elevenlabs.io/v1/text-to-speech/${voiceId}?text=${encodedText}\&xi-api-key=${ELEVENLABS\_API\_KEY}\`;  
  // Using autoplay for immediate playback, but consider user experience  
  return \`\<audio src="${audioUrl}" autoplay controls style="display:none;"/\>\`;  
};

---

### **6\. Proactive Engine Workflow**

The MindFlow Agent's proactive capabilities are crucial to FlowMind AI. This workflow illustrates how it identifies opportunities to assist the user without explicit prompting.

Fragmento de cÃ³digo

sequenceDiagram  
    participant Scheduler  
    participant MindFlow  
    participant DB  
    participant Google  
    participant UI  
      
    Scheduler-\>\>MindFlow: Trigger 'runMindFlowScan' (e.g., every 30 mins)  
    MindFlow-\>\>DB: Query for urgent, pending tasks (TaskFlow logic)  
    MindFlow-\>\>Google: Query user's free time slots from Calendar API  
    Google--\>\>MindFlow: Returns available time slots  
    MindFlow-\>\>DB: Process urgent tasks \+ free slots (find matches)  
    MindFlow-\>\>UI: Send proactive suggestion payload  
    UI-\>\>User: Display "ðŸ’¡ How about working on X at Y?" with "Confirm/Later" buttons  
    User-\>\>UI: User clicks "Confirm"  
    UI-\>\>Bolt.new Engine: Send 'confirm\_suggestion' event  
    Bolt.new Engine-\>\>MindFlow: Route 'confirm\_suggestion' to MindFlow Agent  
    MindFlow-\>\>Google: Create new calendar event for task (via google.js)  
    Google--\>\>MindFlow: Confirmation of event creation  
    MindFlow-\>\>DB: Update task status to 'scheduled'  
    MindFlow-\>\>UI: Send success notification  
    UI-\>\>User: Display "âœ… Task X scheduled\!"

---

### **7\. Development Roadmap (5-Day Sprint)**

This agile roadmap outlines a realistic plan for building the FlowMind AI MVP within a 5-day hackathon sprint, emphasizing parallel development of core features and challenge integrations.

#### **Day 1: Core Setup & Foundation**

1. **Project Initialization:**  
2. Bash

bolt new flowmind-ai \--template=productivity \# Scaffold new Bolt.new project  
bolt auth google \--scopes=https://www.googleapis.com/auth/calendar.events,https://www.googleapis.com/auth/tasks \# Configure Google OAuth with necessary scopes

3.   
4.   
5. **Database Schema Definition:** Define initial Prisma models for Task, Event, Note, and LLMConfig directly within Bolt.new's environment (e.g., server/models/Task.prisma).  
6. Fragmento de cÃ³digo

// server/models/Task.prisma  
model Task {  
  id        String    @id @default(uuid())  
  title     String  
  description String?  
  due\_date  DateTime?  
  priority  String    @default("medium") // 'low', 'medium', 'high'  
  status    String    @default("pending") // 'pending', 'scheduled', 'done', 'cancelled'  
  user\_id   String    // Foreign key to user in Supabase (or Bolt.new's internal user system)  
  created\_at DateTime @default(now())  
  updated\_at DateTime @updatedAt  
}  
// ... similarly for Event, Note, LLMConfig

7.   
8.   
9. **Initial Deployment & Netlify Setup:**  
10. Bash

bolt deploy netlify \--prod \--alias\=flowmind-ai \# First deployment to establish CI/CD pipeline

11.   
12. 

#### **Day 2: Core Agent System & Basic Interactions**

1. **TaskFlow Agent Implementation:** Write declarative prompts for TaskFlow.prompt to handle task creation, completion, and deletion based on user input.  
2. Fragmento de cÃ³digo

\# server/agents/TaskFlow.prompt \- Example  
WHEN user\_input MATCHES "add task {task\_name} due {due\_date}" THEN  
    CREATE Task(title: "{task\_name}", due\_date: parseDate("{due\_date}"), user\_id: current\_user.id)  
    RESPONSE "Task '{task\_name}' scheduled\!"

3.   
4.   
5. **MindFlow Scheduler Setup:** Configure a cron job or scheduled function within server/hooks/cron.js to periodically trigger the MindFlow Agent's proactive scan.  
6. JavaScript

// server/hooks/cron.js  
const cron \= require('node-cron');  
const { runMindFlowScan } \= require('../utils/scheduler');

cron.schedule('\*/30 \* \* \* \*', () \=\> { // Runs every 30 minutes  
  console.log('Running MindFlow proactive scan...');  
  runMindFlowScan(); // This function will contain logic to iterate users and trigger MindFlow.prompt  
});

7.   
8.   
9. **Basic Chat UI:** Ensure the ui/dashboard/chat.html is functional for text input/output.

#### **Day 3: Advanced AI & Voice Integration**

1. **Multi-LLM Router Configuration:** Implement server/utils/llmRouter.js to dynamically route LLM requests based on LLMConfig data, making it easy to switch providers.  
2. JavaScript

// server/utils/llmRouter.js (as provided in section 5.1)

3.   
4.   
5. **ElevenLabs Integration:**  
   * Implement server/integrations/elevenlabs.js for Speech-to-Text and Text-to-Speech API calls.  
   * Add simple microphone/speaker buttons to ui/dashboard/chat.html that trigger these functions, allowing for voice input and spoken responses.  
6. HTML

\<button onclick\="startRecording()"\>ðŸŽ¤ Talk to FlowMind\</button\>  
\<div id\="ai-response-audio"\>\</div\> \<script\>  
    // Client-side JS to capture audio and send to Bolt.new backend for ElevenLabs STT  
    // On response, update \#ai-response-audio with the \<audio\> tag from ElevenLabs TTS  
\</script\>

7.   
8. 

#### **Day 4: Calendar Sync & Proactive Enhancements**

1. **Google Calendar Integration:**  
   * Enhance server/integrations/google.js to fetch free time slots and create new calendar events.  
   * Ensure the CalendarFlow.prompt can interact with these functions.  
2. JavaScript

// server/integrations/google.js (as provided in section 5.2)

3.   
4.   
5. **Proactive Suggestion UX:** Design the UI elements in ui/dashboard/index.html or chat.html to display the proactive suggestions from MindFlow Agent with "Confirm" and "Later" actions.  
6. HTML

\<div id\="proactive-suggestions"\>  
  \<div class\="suggestion-card" data-task-id\="abc"\>  
    \<p\>ðŸ’¡ Schedule "Finish proposal" at 2 PM today?\</p\>  
    \<button class\="confirm-btn" onclick\="confirmSuggestion('abc')"\>Confirm\</button\>  
    \<button class\="later-btn"\>Later\</button\>  
  \</div\>  
\</div\>

7.   
8. 

#### **Day 5: Tavus Video Reports, Testing & Final Deployment**

1. **Tavus Video Report Integration:**  
   * Implement server/integrations/tavus.js to generate personalized video summaries.  
   * Add a trigger (e.g., a "Generate Daily Video Report" button) in the dashboard that calls this integration.  
2. JavaScript

// server/integrations/tavus.js (as provided in section 12\)

3.   
4.   
5. **Comprehensive Testing:** Run all agents.test.js and integrations.test.js to ensure stability and correctness.  
6. Gherkin

\# tests/agents.test.js \- Example Gherkin-like test case for clarity  
Scenario: Proactive scheduling of high-priority task  
  Given a user "JohnDoe" with a high-priority task "Review Q3 Report" due tomorrow  
  And JohnDoe has a 2-hour free slot in his Google Calendar today at 2 PM  
  When the MindFlow Agent's hourly scan runs  
  Then a suggestion "How about working on 'Review Q3 Report' at 2 PM?" should be displayed in JohnDoe's UI  
  And if JohnDoe confirms the suggestion, a new event "Work on Review Q3 Report" should be created in his Google Calendar  
  And the "Review Q3 Report" task status should be updated to 'scheduled' in the database.

7.   
8.   
9. **Final Deployment & Environment Variables:**  
10. Bash

bolt build \# Final build for production  
bolt deploy netlify \--prod \# Deploy final version  
\# Ensure all necessary API keys are set in Netlify environment variables  
netlify env:set ELEVENLABS\_KEY=your\_elevenlabs\_api\_key\_here  
netlify env:set TAVUS\_KEY=your\_tavus\_api\_key\_here  
netlify env:set SUPABASE\_URL=your\_supabase\_url\_here  
netlify env:set SUPABASE\_ANON\_KEY=your\_supabase\_anon\_key\_here  
netlify env:set GOOGLE\_CLIENT\_SECRET=your\_google\_client\_secret\_here  
\# ... and any other sensitive keys

11.   
12. 

---

### **8\. Testing Strategy**

Our testing strategy focuses on validating the **prompt-driven logic** and the **reliability of external API integrations**, ensuring the core functionality of FlowMind AI works as expected.

#### **8.1 Agent Validation Suite**

These tests confirm that our prompt-based agents (TaskFlow, MindFlow) interpret commands and execute actions correctly within Bolt.new's environment.

JavaScript

// tests/agents.test.js  
const { simulateInput, createTask, getActiveLLMConfig, runMindFlowScan } \= require('../bolt.new/server/utils/test-helpers'); // Mock helpers  
const prisma \= require('../bolt.new/server/models'); // Direct access to Bolt.new's Prisma client

describe('TaskFlow Agent Logic', () \=\> {  
  beforeEach(async () \=\> {  
    await prisma.task.deleteMany({}); // Clean DB before each test  
  });

  test('TaskFlow creates a new task correctly from user input', async () \=\> {  
    const tomorrow \= new Date();  
    tomorrow.setDate(tomorrow.getDate() \+ 1);  
    const tomorrowStr \= tomorrow.toISOString().split('T')\[0\]; // YYYY-MM-DD

    const response \= await simulateInput(\`add task Buy groceries for ${tomorrowStr}\`);  
    const task \= await prisma.task.findFirst({ where: { title: "Buy groceries" } });

    expect(task).not.toBeNull();  
    expect(task.title).toBe("Buy groceries");  
    expect(task.status).toBe("pending");  
    expect(response).toContain("âœ… Task 'Buy groceries' added");  
  });

  test('TaskFlow marks an existing task as complete', async () \=\> {  
    await createTask("Write proposal", { status: "pending", user\_id: "test-user-1" });  
    const response \= await simulateInput("complete Write proposal", "test-user-1");  
    const task \= await prisma.task.findFirst({ where: { title: "Write proposal" } });

    expect(task.status).toBe("done");  
    expect(response).toContain("ðŸŽ‰ Great job\! 'Write proposal' marked as complete.");  
  });  
});

describe('MindFlow Agent Proactive Logic', () \=\> {  
  beforeEach(async () \=\> {  
    await prisma.task.deleteMany({});  
    // Mock Google API calls for free slots  
    jest.spyOn(require('../bolt.new/server/integrations/google'), 'getFreeCalendarSlots').mockResolvedValue(\[  
      { start: new Date(Date.now() \+ 2 \* 60 \* 60 \* 1000).toISOString(), duration: 2 \* 60 \* 60 \* 1000 } // Free slot 2 hours from now for 2 hours  
    \]);  
  });

  afterEach(() \=\> {  
    jest.restoreAllMocks(); // Clean up mocks  
  });

  test('MindFlow suggests scheduling a high-priority, urgent task in a free slot', async () \=\> {  
    const dueTomorrow \= new Date();  
    dueTomorrow.setDate(dueTomorrow.getDate() \+ 1);  
    await createTask("Urgent Project", { priority: "high", due\_date: dueTomorrow, user\_id: "test-user-1" });

    const suggestions \= await runMindFlowScan("test-user-1"); // Simulate scan for a specific user  
      
    // Expect a suggestion for "Urgent Project"  
    expect(suggestions).toContain("ðŸ’¡ How about working on 'Urgent Project'");  
  });  
});

#### **8.2 Integration Tests**

These tests verify the proper communication and data exchange with external services like Google APIs and ElevenLabs.

JavaScript

// tests/integrations.test.js  
const { createCalendarEvent, getFreeCalendarSlots } \= require('../bolt.new/server/integrations/google');  
const { handleVoiceInput, speakResponse } \= require('../bolt.new/server/integrations/elevenlabs');

describe('Google API Integration', () \=\> {  
  test('createCalendarEvent successfully creates an event', async () \=\> {  
    // Mock Google API response for event insertion  
    const mockInsertResponse \= { data: { id: 'event123', summary: 'Test Event', start: { dateTime: '...' } } };  
    jest.spyOn(require('bolt-google-auth').google.calendar('v3').events, 'insert').mockResolvedValue(mockInsertResponse);

    const eventDetails \= {  
      title: "Test Event",  
      start\_time: new Date().toISOString(),  
      end\_time: new Date(Date.now() \+ 60 \* 60 \* 1000).toISOString()  
    };  
    const createdEvent \= await createCalendarEvent("test-user-id", eventDetails);

    expect(createdEvent.id).toBe('event123');  
    expect(createdEvent.summary).toBe('Test Event');  
  });

  test('getFreeCalendarSlots returns expected free slots (mocked)', async () \=\> {  
    const startTime \= new Date();  
    const endTime \= new Date(startTime.getTime() \+ 24 \* 60 \* 60 \* 1000); // 24 hours later  
    const slots \= await getFreeCalendarSlots("test-user-id", startTime, endTime, "1h");

    expect(slots.length).toBeGreaterThan(0);  
    expect(slots\[0\]).toHaveProperty('start');  
    expect(slots\[0\]).toHaveProperty('duration');  
    expect(slots\[0\].duration).toBeGreaterThanOrEqual(1 \* 60 \* 60 \* 1000); // At least 1 hour duration  
  });  
});

describe('ElevenLabs Integration', () \=\> {  
  const mockAudioBlob \= new Blob(\["mock audio data"\], { type: "audio/wav" });

  test('handleVoiceInput transcribes audio correctly', async () \=\> {  
    // Mock ElevenLabs STT API response  
    global.fetch \= jest.fn(() \=\>  
      Promise.resolve({  
        ok: true,  
        json: () \=\> Promise.resolve({ text: "Hello FlowMind" }),  
        text: () \=\> Promise.resolve(""), // For error cases  
      })  
    );

    const transcribedText \= await handleVoiceInput(mockAudioBlob);  
    expect(transcribedText).toBe("Hello FlowMind");  
    expect(fetch).toHaveBeenCalledWith('https://api.elevenlabs.io/v1/speech-to-text', expect.any(Object));  
  });

  test('speakResponse generates correct audio HTML tag', () \=\> {  
    const audioHtml \= speakResponse("How can I help you?");  
    expect(audioHtml).toContain('\<audio');  
    expect(audioHtml).toContain('autoplay');  
    expect(audioHtml).toContain('https://api.elevenlabs.io/v1/text-to-speech/');  
    expect(audioHtml).toContain('text=How%20can%20I%20help%20you%3F');  
  });  
});

---

### **9\. Deployment Architecture**

FlowMind AI's deployment leverages **Netlify for seamless CI/CD** directly integrated with Bolt.new, ensuring a robust, scalable, and easy-to-manage production environment.

Fragmento de cÃ³digo

graph TB  
    A\[GitHub Repository\<br\>(Source Code)\] \--\> B\[Netlify\<br\>(Automated Build & Deploy)\]  
    B \--\> C\[Bolt.new Builder\<br\>(Generates Optimized App)\]  
    C \--\> D\[Production Deployment\<br\>(Global CDN, Serverless Functions)\]  
    D \--\> E\[PostgreSQL DB\<br\>(Bolt.new Internal / Managed)\]  
    D \--\> F\[Supabase\<br\>(External Auth & DB for Scale)\]  
    D \--\> G\[Google APIs\<br\>(OAuth, Calendar, Tasks)\]  
    D \--\> H\[AI Services\<br\>(ElevenLabs, Tavus, LLM Providers)\]  
      
    style D fill:\#4CAF50,stroke:\#388E3C,stroke-width:2px;  
    classDef external fill:\#f9f9f9,stroke:\#333;  
    class E,F,G,H external;

**Deployment Process:**

1. **Build Project:** Bolt.new compiles and optimizes the application for production.  
2. Bash

bolt build

3.   
4.   
5. **Deploy to Netlify:** This command initiates a deploy through Bolt.new's integrated Netlify CLI, automatically configuring CI/CD.  
6. Bash

bolt deploy netlify \--prod \--alias\=flowmind-ai \# Deploys to flowmind-ai.netlify.app

7.   
8.   
9. **Set Environment Variables:** Critical API keys and configuration settings are securely managed as environment variables within the Netlify dashboard.  
10. Bash

netlify env:set ELEVENLABS\_KEY=xxx\_your\_elevenlabs\_api\_key\_xxx  
netlify env:set TAVUS\_KEY=yyy\_your\_tavus\_api\_key\_yyy  
netlify env:set SUPABASE\_URL=zzz\_your\_supabase\_project\_url\_zzz  
netlify env:set SUPABASE\_ANON\_KEY=aaa\_your\_supabase\_anon\_key\_aaa  
netlify env:set GOOGLE\_CLIENT\_ID=bbb\_your\_google\_client\_id\_bbb  
netlify env:set GOOGLE\_CLIENT\_SECRET=ccc\_your\_google\_client\_secret\_ccc  
\# ... ensure all required keys are set for both Bolt.new and integrated services

11.   
12. 

---

### **10\. Error Handling & Monitoring**

Robust error handling and monitoring are critical for a production-ready application. FlowMind AI incorporates several layers to ensure reliability and a smooth user experience.

#### **10.1 Schema Validation (via Prisma)**

Prisma models allow for powerful schema-level validation, ensuring data integrity before it even hits the database.

Fragmento de cÃ³digo

// bolt.new/server/models/Task.prisma  
model Task {  
  id        String    @id @default(uuid())  
  title     String    @db.VarChar(255) // Ensures title is a string and max length  
  due\_date  DateTime  @futureOnly // Custom validation: due\_date must be in the future (requires Prisma extension or Bolt.new feature)  
  priority  String    @validate(\['low','medium','high'\]) // Custom validation: restricts priority values  
  status    String    @default("pending") @validate(\['pending', 'scheduled', 'done', 'cancelled'\]) // Restricts status values  
  user\_id   String  
  created\_at DateTime @default(now())  
  updated\_at DateTime @updatedAt  
}

*Note: @futureOnly and @validate are illustrative of desired validation capabilities; their direct implementation depends on Bolt.new's Prisma integration features.*

#### **10.2 API Fallback System**

For external AI services, a simple fallback mechanism can be implemented in llmRouter.js and other integration files to maintain functionality even if a primary service experiences issues.

JavaScript

// bolt.new/server/utils/llmRouter.js \- Enhanced with fallback  
const prisma \= require('../models');

async function routeLLM(promptMessages, primaryAttempt \= true) {  
  const config \= await prisma.lLMConfig.findFirst({  
    where: { is\_active: true }  
  });

  if (\!config) {  
    console.error("No active LLM configuration found.");  
    throw new Error("AI service unavailable.");  
  }

  const payload \= {  
    model: config.model\_name,  
    messages: promptMessages  
  };

  try {  
    const response \= await fetch(config.endpoint, {  
      method: "POST",  
      headers: {  
        "Content-Type": "application/json",  
        "Authorization": \`Bearer ${config.api\_key}\`  
      },  
      body: JSON.stringify(payload)  
    });

    if (\!response.ok) {  
      throw new Error(\`LLM API failed with status ${response.status}: ${await response.text()}\`);  
    }

    const data \= await response.json();  
    return data.choices\[0\].message.content;  
  } catch (error) {  
    console.error(\`Error calling primary LLM (${config.model\_name}):\`, error);  
    if (primaryAttempt) {  
      // Logic to switch to a backup LLM or notify admin  
      console.warn("Attempting fallback to secondary LLM service...");  
      await prisma.lLMConfig.updateMany({ // Deactivate current, activate backup (simplified)  
        where: { is\_active: true, id: config.id }, data: { is\_active: false }  
      });  
      // In a real scenario, this would choose another available LLM  
      // For MVP, we might just re-attempt with a predefined secondary, or return a graceful error.  
      // For now, let's just re-throw after logging.  
      throw new Error("Primary AI service failed, attempting fallback is beyond MVP scope. Please try again.");  
    } else {  
      throw error; // If fallback also fails  
    }  
  }  
}

#### **10.3 User Notifications (Frontend)**

Implement a simple toast notification system to provide immediate feedback to the user regarding system status or errors.

JavaScript

// bolt.new/ui/utils/notifications.js (This would be part of Bolt.new's generated UI utilities)  
exports.showToast \= (options) \=\> {  
  const { title, message, type, timeout \= 3000 } \= options;  
  const toastContainer \= document.getElementById('toast-container') || (() \=\> {  
    const div \= document.createElement('div');  
    div.id \= 'toast-container';  
    document.body.appendChild(div);  
    return div;  
  })();

  const toast \= document.createElement('div');  
  toast.className \= \`toast toast--${type}\`; // e.g., 'toast toast--warning'  
  toast.innerHTML \= \`\<strong\>${title}\</strong\>\<p\>${message}\</p\>\`;  
  toastContainer.appendChild(toast);

  setTimeout(() \=\> {  
    toast.remove();  
  }, timeout);  
};

// Example usage in UI-related Bolt.new component or client-side script:  
// ui.showToast({  
//   title: "AI Service Alert",  
//   message: "Using backup AI service due to primary outage.",  
//   type: "warning",  
//   timeout: 5000  
// });

---

### **11\. Complete Agent Logic Map**

This table summarizes the responsibilities and interactions of each AI agent within FlowMind AI, highlighting their prompt-driven nature.

| Agent | Primary Trigger | Core Actions | Key Data Sources | Purpose / Output |
| :---- | :---- | :---- | :---- | :---- |
| **TaskFlow** | User text command (e.g., "add task X") | CRUD operations (Create, Read, Update, Delete) on tasks. | Bolt.new's PostgreSQL DB (via Prisma) | Efficient task management, direct user responses. |
| **CalendarFlow** | User text command (e.g., "schedule meeting", "what's tomorrow?") | Create/Read calendar events, query free/busy times. | Google Calendar API | Seamless calendar integration, scheduling assistance. |
| **InfoFlow** | User text query (e.g., "explain X", "summarize Y") | Process natural language, query external LLMs for information/summaries. | Dynamic LLM Router (OpenAI, Gemini, Claude) | Provide concise information, summaries, and answers to general questions. |
| **MindFlow** | Scheduled intervals (e.g., every 30 mins) | Proactive identification of urgent tasks, matching with free calendar slots, suggesting actions. | Bolt.new's PostgreSQL DB (Tasks), Google Calendar API (Free Slots) | Anticipate user needs, offer intelligent suggestions, automate workflows (e.g., scheduling). **The 'proactive' core.** |

---

### **12\. Video Report Generation**

For the Conversational AI Video Challenge, FlowMind AI will use Tavus to generate personalized video summaries, offering a novel way to consume productivity insights.

Fragmento de cÃ³digo

graph LR  
    A\[MindFlow Agent\<br\>(Generates Summary Text)\] \--\> B\[Bolt.new Server Function\<br\>(Triggers Tavus Integration)\]  
    B \--\> C\[Tavus API\<br\>(Generates Personalized Video)\]  
    C \--\> D\[Video URL\<br\>(Returned to Bolt.new)\]  
    D \--\> E\[Embed in Dashboard\<br\>(UI)\]  
    E \--\> F\[User Views Personalized\<br\>Video Productivity Report\]

Implementation Details (server/integrations/tavus.js):

This function takes a generated text summary and user context, sends it to Tavus, and returns the video URL to be embedded in the UI.

JavaScript

// bolt.new/server/integrations/tavus.js  
const TAVUS\_API\_KEY \= process.env.TAVUS\_KEY;

/\*\*  
 \* Generates a personalized video report using Tavus.  
 \* @param {string} summaryText \- The text summary to be spoken by the AI avatar.  
 \* @param {object} userDetails \- Contains user info (e.g., name) for personalization.  
 \* @returns {Promise\<string\>} The URL of the generated video.  
 \*/  
exports.generateDailyReportVideo \= async (summaryText, userDetails) \=\> {  
  try {  
    const response \= await fetch('https://api.tavus.io/videos', {  
      method: 'POST',  
      headers: {  
        'Content-Type': 'application/json',  
        'Authorization': \`Bearer ${TAVUS\_API\_KEY}\`  
      },  
      body: JSON.stringify({  
        template\_id: 'your\_predefined\_tavus\_template\_id', // A template created in Tavus for daily reports  
        data: {  
          summary\_text: summaryText,  
          user\_name: userDetails.name || 'Valued User'  
        },  
        // Optional: specify avatar, voice, etc., if not part of the template  
        // avatar\_id: '...',  
        // voice\_id: '...',  
      })  
    });

    if (\!response.ok) {  
      const errorBody \= await response.text();  
      console.error('Tavus API error:', errorBody);  
      throw new Error(\`Failed to generate video report: ${response.status} \- ${errorBody}\`);  
    }

    const data \= await response.json();  
    return data.video\_url; // Tavus typically returns a URL to the generated video  
  } catch (error) {  
    console.error('Error generating Tavus video:', error);  
    throw error;  
  }  
};

// Example usage within a Bolt.new server function or MindFlow agent action:  
// async function createAndEmbedVideoReport(user) {  
//   const urgentTasks \= await getUrgentTasks(user); // Logic to get tasks  
//   const dailySummary \= await mindFlow.generateSummary(urgentTasks); // LLM-generated summary  
//   const videoUrl \= await generateDailyReportVideo(dailySummary, { name: user.name });  
//   // Then, update the UI to display or embed this videoUrl for the user.  
//   // This might involve updating a 'user\_settings' table with the videoUrl,  
//   // and the UI polling or being notified to display it.  
// }

---

This comprehensive technical proposal details FlowMind AI's innovative approach to personal productivity. It covers:

1. **A complete architecture with Bolt.new** at its core, showcasing how intelligent agents are orchestrated.  
2. **A detailed file structure** that reflects a minimal-code development philosophy.  
3. **A clear technology stack implementation**, aligning seamlessly with all hackathon challenges.  
4. **Ready-to-use code fragments** for critical, low-code functionality.  
5. **Visual workflows** for key processes, illustrating the system's dynamic behavior.  
6. **A day-by-day development roadmap** for agile implementation during the sprint.  
7. **Robust testing and deployment strategies** to ensure a high-quality, production-ready application.  
8. **Error handling and monitoring approaches** for resilience.

All components are optimized for Bolt.new's no-code and low-code capabilities, promising a powerful, proactive, and unique productivity assistant that meets all challenge criteria.

We are excited to bring FlowMind AI to life\!

